{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rmiW2qEOY9WG",
        "outputId": "94ffd1f8-85e7-4999-98b8-72a96bf47058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 50\n",
            "========== Loading data ==========\n",
            "========== Vectorizing data ==========\n",
            "========== Training for 10 epochs ==========\n",
            "Training started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:31<00:00, 15.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 1\n",
            "Training accuracy for epoch 1: 0.527\n",
            "Training time for this epoch: 31.78461742401123\n",
            "Validation started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 73.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 1\n",
            "Validation accuracy for epoch 1: 0.535\n",
            "Validation time for this epoch: 0.6853542327880859\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:33<00:00, 14.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 2\n",
            "Training accuracy for epoch 2: 0.585625\n",
            "Training time for this epoch: 33.916747093200684\n",
            "Validation started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 73.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 2\n",
            "Validation accuracy for epoch 2: 0.59375\n",
            "Validation time for this epoch: 0.6837759017944336\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:41<00:00, 12.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 3\n",
            "Training accuracy for epoch 3: 0.6225\n",
            "Training time for this epoch: 41.35323095321655\n",
            "Validation started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 71.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 3\n",
            "Validation accuracy for epoch 3: 0.59625\n",
            "Validation time for this epoch: 0.7080729007720947\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:41<00:00, 12.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 4\n",
            "Training accuracy for epoch 4: 0.64525\n",
            "Training time for this epoch: 41.61332058906555\n",
            "Validation started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 60.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 4\n",
            "Validation accuracy for epoch 4: 0.59625\n",
            "Validation time for this epoch: 0.8300416469573975\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 5\n",
            "Training accuracy for epoch 5: 0.651\n",
            "Training time for this epoch: 44.21337389945984\n",
            "Validation started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 69.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 5\n",
            "Validation accuracy for epoch 5: 0.5975\n",
            "Validation time for this epoch: 0.7204208374023438\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:43<00:00, 11.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 6\n",
            "Training accuracy for epoch 6: 0.684375\n",
            "Training time for this epoch: 43.51960206031799\n",
            "Validation started for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 71.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 6\n",
            "Validation accuracy for epoch 6: 0.61\n",
            "Validation time for this epoch: 0.7069120407104492\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:43<00:00, 11.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 7\n",
            "Training accuracy for epoch 7: 0.718375\n",
            "Training time for this epoch: 43.516746520996094\n",
            "Validation started for epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 70.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 7\n",
            "Validation accuracy for epoch 7: 0.52625\n",
            "Validation time for this epoch: 0.7121109962463379\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:44<00:00, 11.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 8\n",
            "Training accuracy for epoch 8: 0.726125\n",
            "Training time for this epoch: 44.588974714279175\n",
            "Validation started for epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 71.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 8\n",
            "Validation accuracy for epoch 8: 0.5925\n",
            "Validation time for this epoch: 0.7013015747070312\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:44<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 9\n",
            "Training accuracy for epoch 9: 0.752\n",
            "Training time for this epoch: 44.41434288024902\n",
            "Validation started for epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 70.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 9\n",
            "Validation accuracy for epoch 9: 0.6025\n",
            "Validation time for this epoch: 0.7104687690734863\n",
            "Results saved to results_rnn_hidden50.json\n",
            "Training started for epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:44<00:00, 11.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed for epoch 10\n",
            "Training accuracy for epoch 10: 0.78025\n",
            "Training time for this epoch: 44.09694576263428\n",
            "Validation started for epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 65.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 10\n",
            "Validation accuracy for epoch 10: 0.59\n",
            "Validation time for this epoch: 0.7714321613311768\n",
            "Results saved to results_rnn_hidden50.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "\n",
        "unk = '<UNK>'\n",
        "# Consult the PyTorch documentation for information on the functions used below:\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, h):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.h = h\n",
        "        self.W1 = nn.Linear(input_dim, h)\n",
        "        self.activation = nn.ReLU() # The rectified linear unit; one valid choice of activation function\n",
        "        self.output_dim = 5\n",
        "        self.W2 = nn.Linear(h, self.output_dim)\n",
        "\n",
        "        self.softmax = nn.LogSoftmax() # The softmax function that converts vectors into probability distributions; computes log probabilities for computational benefits\n",
        "        self.loss = nn.NLLLoss() # The cross-entropy/negative log likelihood loss taught in class\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label):\n",
        "        return self.loss(predicted_vector, gold_label)\n",
        "\n",
        "    def forward(self, input_vector):\n",
        "        # [to fill] obtain first hidden layer representation\n",
        "        hidden_layer = self.activation(self.W1(input_vector))\n",
        "        # [to fill] obtain output layer representation\n",
        "        output_layer = self.W2(hidden_layer)\n",
        "        # [to fill] obtain probability dist.\n",
        "        predicted_vector = self.softmax(output_layer)\n",
        "        # return predicted probability distribution\n",
        "        return predicted_vector\n",
        "\n",
        "\n",
        "# Returns:\n",
        "# vocab = A set of strings corresponding to the vocabulary\n",
        "def make_vocab(data):\n",
        "    vocab = set()\n",
        "    for document, _ in data:\n",
        "        for word in document:\n",
        "            vocab.add(word)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "# Returns:\n",
        "# vocab = A set of strings corresponding to the vocabulary including <UNK>\n",
        "# word2index = A dictionary mapping word/token to its index (a number in 0, ..., V - 1)\n",
        "# index2word = A dictionary inverting the mapping of word2index\n",
        "def make_indices(vocab):\n",
        "    vocab_list = sorted(vocab)\n",
        "    vocab_list.append(unk)\n",
        "    word2index = {}\n",
        "    index2word = {}\n",
        "    for index, word in enumerate(vocab_list):\n",
        "        word2index[word] = index\n",
        "        index2word[index] = word\n",
        "    vocab.add(unk)\n",
        "    return vocab, word2index, index2word\n",
        "\n",
        "\n",
        "# Returns:\n",
        "# vectorized_data = A list of pairs (vector representation of input, y)\n",
        "def convert_to_vector_representation(data, word2index):\n",
        "    vectorized_data = []\n",
        "    for document, y in data:\n",
        "        vector = torch.zeros(len(word2index))\n",
        "        for word in document:\n",
        "            index = word2index.get(word, word2index[unk])\n",
        "            vector[index] += 1\n",
        "        vectorized_data.append((vector, y))\n",
        "    return vectorized_data\n",
        "\n",
        "\n",
        "\n",
        "def load_data(train_data, val_data):\n",
        "    with open(train_data) as training_f:\n",
        "        training = json.load(training_f)\n",
        "    with open(val_data) as valid_f:\n",
        "        validation = json.load(valid_f)\n",
        "\n",
        "    tra = []\n",
        "    val = []\n",
        "    for elt in training:\n",
        "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
        "    for elt in validation:\n",
        "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
        "\n",
        "    return tra, val\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = ArgumentParser()\n",
        "    # parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, default=50, required = True, help = \"hidden_dim\")\n",
        "    # parser.add_argument(\"-e\", \"--epochs\", type=int, default=10, required = True, help = \"num of epochs to train\")\n",
        "    # parser.add_argument(\"--train_data\", required = True, default=\"training.json\", help = \"path to training data\")\n",
        "    # parser.add_argument(\"--val_data\", required = True, default=\"validation.json\", help = \"path to validation data\")\n",
        "    hidden_dim = 50\n",
        "    epochs = 10\n",
        "    train_data_file = \"training.json\"\n",
        "    val_data_file = \"validation.json\"\n",
        "    output_file = f\"results_rnn_hidden{hidden_dim}.json\"\n",
        "    parser.add_argument(\"--test_data\", default = \"to fill\", help = \"path to test data\")\n",
        "    parser.add_argument('--do_train', action='store_true')\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    args.hidden_dim = hidden_dim\n",
        "    args.epochs = epochs\n",
        "    args.train_data = train_data_file\n",
        "    args.val_data = val_data_file\n",
        "\n",
        "    # fix random seeds\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    #Hidden Dimension\n",
        "    print(\"Hidden Dimension: {}\".format(args.hidden_dim))\n",
        "\n",
        "    # load data\n",
        "    print(\"========== Loading data ==========\")\n",
        "    train_data, valid_data = load_data(args.train_data, args.val_data) # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n",
        "    vocab = make_vocab(train_data)\n",
        "    vocab, word2index, index2word = make_indices(vocab)\n",
        "\n",
        "    print(\"========== Vectorizing data ==========\")\n",
        "    train_data = convert_to_vector_representation(train_data, word2index)\n",
        "    valid_data = convert_to_vector_representation(valid_data, word2index)\n",
        "\n",
        "\n",
        "    model = FFNN(input_dim = len(vocab), h = args.hidden_dim)\n",
        "    optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
        "    results = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": [], \"val_loss\": [], \"train_time\": [], \"val_time\": []}\n",
        "    print(\"========== Training for {} epochs ==========\".format(args.epochs))\n",
        "    for epoch in range(args.epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss = None\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        start_time = time.time()\n",
        "        print(\"Training started for epoch {}\".format(epoch + 1))\n",
        "        random.shuffle(train_data) # Good practice to shuffle order of training data\n",
        "        minibatch_size = 16\n",
        "        N = len(train_data)\n",
        "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
        "            optimizer.zero_grad()\n",
        "            loss = None\n",
        "            for example_index in range(minibatch_size):\n",
        "                input_vector, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
        "                predicted_vector = model(input_vector)\n",
        "                predicted_label = torch.argmax(predicted_vector)\n",
        "                correct += int(predicted_label == gold_label)\n",
        "                total += 1\n",
        "                example_loss = model.compute_Loss(predicted_vector.view(1,-1), torch.tensor([gold_label]))\n",
        "                if loss is None:\n",
        "                    loss = example_loss\n",
        "                else:\n",
        "                    loss += example_loss\n",
        "            loss = loss / minibatch_size\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(\"Training completed for epoch {}\".format(epoch + 1))\n",
        "        print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
        "        print(\"Training time for this epoch: {}\".format(time.time() - start_time))\n",
        "        results[\"train_acc\"].append(float(correct) / float(total))\n",
        "        results[\"train_loss\"].append(float(loss.item()))  # convert tensor to float\n",
        "        results[\"train_time\"].append(float(time.time() - start_time))\n",
        "\n",
        "\n",
        "        loss = None\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        start_time = time.time()\n",
        "        print(\"Validation started for epoch {}\".format(epoch + 1))\n",
        "        minibatch_size = 16\n",
        "        N = len(valid_data)\n",
        "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
        "            optimizer.zero_grad()\n",
        "            loss = None\n",
        "            for example_index in range(minibatch_size):\n",
        "                input_vector, gold_label = valid_data[minibatch_index * minibatch_size + example_index]\n",
        "                predicted_vector = model(input_vector)\n",
        "                predicted_label = torch.argmax(predicted_vector)\n",
        "                correct += int(predicted_label == gold_label)\n",
        "                total += 1\n",
        "                example_loss = model.compute_Loss(predicted_vector.view(1,-1), torch.tensor([gold_label]))\n",
        "                if loss is None:\n",
        "                    loss = example_loss\n",
        "                else:\n",
        "                    loss += example_loss\n",
        "            loss = loss / minibatch_size\n",
        "        print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
        "        print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
        "        print(\"Validation time for this epoch: {}\".format(time.time() - start_time))\n",
        "        results[\"val_acc\"].append(float(correct) / float(total))\n",
        "        results[\"val_loss\"].append(float(loss.item()))  # convert tensor to Python float\n",
        "        results[\"val_time\"].append(float(time.time() - start_time))\n",
        "\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "          json.dump(results, f, indent=2)\n",
        "\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "\n",
        "    # write out to results/test.out\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cu81GeerFO1",
        "outputId": "43e13ac2-8d35-47f6-aa55-80e30cdd884a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import string\n",
        "from argparse import ArgumentParser\n",
        "import pickle\n",
        "\n",
        "unk = '<UNK>'\n",
        "# Consult the PyTorch documentation for information on the functions used below:\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, h):  # Add relevant parameters\n",
        "        super(RNN, self).__init__()\n",
        "        self.h = h\n",
        "        self.numOfLayer = 1\n",
        "        self.rnn = nn.RNN(input_dim, h, self.numOfLayer, nonlinearity='tanh')\n",
        "        self.W = nn.Linear(h, 5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label):\n",
        "        return self.loss(predicted_vector, gold_label)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "         # [to fill] obtain hidden layer representation (https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
        "        _, hidden = self.rnn(inputs)\n",
        "        # [to fill] obtain output layer representations\n",
        "        predicted_vector = self.W(hidden)\n",
        "        # [to fill] sum over output\n",
        "        predicted_vector = torch.sum(predicted_vector, dim=0)\n",
        "        # [to fill] obtain probability dist.\n",
        "        predicted_vector = self.softmax(predicted_vector)\n",
        "        return predicted_vector\n",
        "\n",
        "\n",
        "def load_data(train_data, val_data):\n",
        "    with open(train_data) as training_f:\n",
        "        training = json.load(training_f)\n",
        "    with open(val_data) as valid_f:\n",
        "        validation = json.load(valid_f)\n",
        "\n",
        "    tra = []\n",
        "    val = []\n",
        "    for elt in training:\n",
        "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
        "    for elt in validation:\n",
        "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
        "    return tra, val\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = ArgumentParser()\n",
        "    # parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, default=50, required = True, help = \"hidden_dim\")\n",
        "    # parser.add_argument(\"-e\", \"--epochs\", type=int, default=10, required = True, help = \"num of epochs to train\")\n",
        "    # parser.add_argument(\"--train_data\", required = True, default=\"training.json\", help = \"path to training data\")\n",
        "    # parser.add_argument(\"--val_data\", required = True, default=\"validation.json\", help = \"path to validation data\")\n",
        "    hidden_dim = 100\n",
        "    epochs = 10\n",
        "    train_data_file = \"training.json\"\n",
        "    val_data_file = \"validation.json\"\n",
        "    output_file = f\"results_rnn_hidden{hidden_dim}.json\"\n",
        "    parser.add_argument(\"--test_data\", default = \"to fill\", help = \"path to test data\")\n",
        "    parser.add_argument('--do_train', action='store_true')\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    args.hidden_dim = hidden_dim\n",
        "    args.epochs = epochs\n",
        "    args.train_data = train_data_file\n",
        "    args.val_data = val_data_file\n",
        "    print(\"========== Loading data ==========\")\n",
        "    train_data, valid_data = load_data(args.train_data, args.val_data) # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n",
        "\n",
        "    # Think about the type of function that an RNN describes. To apply it, you will need to convert the text data into vector representations.\n",
        "    # Further, think about where the vectors will come from. There are 3 reasonable choices:\n",
        "    # 1) Randomly assign the input to vectors and learn better embeddings during training; see the PyTorch documentation for guidance\n",
        "    # 2) Assign the input to vectors using pretrained word embeddings. We recommend any of {Word2Vec, GloVe, FastText}. Then, you do not train/update these embeddings.\n",
        "    # 3) You do the same as 2) but you train (this is called fine-tuning) the pretrained embeddings further.\n",
        "    # Option 3 will be the most time consuming, so we do not recommend starting with this\n",
        "\n",
        "    print(\"========== Vectorizing data ==========\")\n",
        "    model = RNN(50, args.hidden_dim)  # Fill in parameters\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    word_embedding = pickle.load(open('./word_embedding.pkl', 'rb'))\n",
        "\n",
        "    stopping_condition = False\n",
        "    epoch = 0\n",
        "\n",
        "    last_train_accuracy = 0\n",
        "    last_validation_accuracy = 0\n",
        "\n",
        "    results = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": [], \"val_loss\": [], \"train_time\": [], \"val_time\": []}\n",
        "    print(\"========== Training ==========\")\n",
        "    while not stopping_condition:\n",
        "        random.shuffle(train_data)\n",
        "        model.train()\n",
        "        # You will need further code to operationalize training, ffnn.py may be helpful\n",
        "        print(\"Training started for epoch {}\".format(epoch + 1))\n",
        "        train_data = train_data\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        minibatch_size = 16\n",
        "        N = len(train_data)\n",
        "\n",
        "        loss_total = 0\n",
        "        loss_count = 0\n",
        "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
        "            optimizer.zero_grad()\n",
        "            loss = None\n",
        "            for example_index in range(minibatch_size):\n",
        "                input_words, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
        "                input_words = \" \".join(input_words)\n",
        "\n",
        "                # Remove punctuation\n",
        "                input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "\n",
        "                # Look up word embedding dictionary\n",
        "                vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i in input_words ]\n",
        "\n",
        "                # Transform the input into required shape\n",
        "                vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
        "                output = model(vectors)\n",
        "\n",
        "                # Get loss\n",
        "                example_loss = model.compute_Loss(output.view(1,-1), torch.tensor([gold_label]))\n",
        "\n",
        "                # Get predicted label\n",
        "                predicted_label = torch.argmax(output)\n",
        "\n",
        "                correct += int(predicted_label == gold_label)\n",
        "                # print(predicted_label, gold_label)\n",
        "                total += 1\n",
        "                if loss is None:\n",
        "                    loss = example_loss\n",
        "                else:\n",
        "                    loss += example_loss\n",
        "\n",
        "            loss = loss / minibatch_size\n",
        "            loss_total += loss.data\n",
        "            loss_count += 1\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(loss_total/loss_count)\n",
        "        print(\"Training completed for epoch {}\".format(epoch + 1))\n",
        "        print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
        "        trainning_accuracy = correct/total\n",
        "        results[\"train_acc\"].append(float(correct) / float(total))\n",
        "        results[\"train_loss\"].append(float(loss.item()))  # convert tensor to float\n",
        "        results[\"train_time\"].append(float(time.time() - start_time))\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        random.shuffle(valid_data)\n",
        "        print(\"Validation started for epoch {}\".format(epoch + 1))\n",
        "        valid_data = valid_data\n",
        "\n",
        "        for input_words, gold_label in tqdm(valid_data):\n",
        "            input_words = \" \".join(input_words)\n",
        "            input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "            vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i\n",
        "                       in input_words]\n",
        "\n",
        "            vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
        "            output = model(vectors)\n",
        "            predicted_label = torch.argmax(output)\n",
        "            correct += int(predicted_label == gold_label)\n",
        "            total += 1\n",
        "            # print(predicted_label, gold_label)\n",
        "        print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
        "        print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
        "        validation_accuracy = correct/total\n",
        "\n",
        "        if validation_accuracy < last_validation_accuracy and trainning_accuracy < last_train_accuracy:\n",
        "            stopping_condition=True\n",
        "            print(\"Training done to avoid overfitting!\")\n",
        "            print(\"Best validation accuracy is:\", last_validation_accuracy)\n",
        "            results[\"val_acc\"].append(float(correct) / float(total))\n",
        "            results[\"val_loss\"].append(float(loss.item()))  # convert tensor to Python float\n",
        "            results[\"val_time\"].append(float(time.time() - start_time))\n",
        "        else:\n",
        "            last_validation_accuracy = validation_accuracy\n",
        "            last_train_accuracy = trainning_accuracy\n",
        "            results[\"val_acc\"].append(float(correct) / float(total))\n",
        "            results[\"val_loss\"].append(float(loss.item()))  # convert tensor to Python float\n",
        "            results[\"val_time\"].append(float(time.time() - start_time))\n",
        "        with open(output_file, \"w\") as f:\n",
        "          json.dump(results, f, indent=2)\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "\n",
        "\n",
        "    # You may find it beneficial to keep track of training accuracy or training loss;\n",
        "\n",
        "    # Think about how to update the model and what this entails. Consider ffnn.py and the PyTorch documentation for guidance\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbzZ6HF51OtO",
        "outputId": "fb4c6fb2-2769-4837-cec8-a80720c0ba3b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Loading data ==========\n",
            "========== Vectorizing data ==========\n",
            "========== Training ==========\n",
            "Training started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [02:06<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1247)\n",
            "Training completed for epoch 1\n",
            "Training accuracy for epoch 1: 0.40125\n",
            "Validation started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:04<00:00, 179.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 1\n",
            "Validation accuracy for epoch 1: 0.4\n",
            "Training started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [02:05<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1235)\n",
            "Training completed for epoch 2\n",
            "Training accuracy for epoch 2: 0.4045\n",
            "Validation started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:03<00:00, 209.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 2\n",
            "Validation accuracy for epoch 2: 0.4225\n",
            "Training started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [02:04<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1122)\n",
            "Training completed for epoch 3\n",
            "Training accuracy for epoch 3: 0.41\n",
            "Validation started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:04<00:00, 169.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 3\n",
            "Validation accuracy for epoch 3: 0.4075\n",
            "Training started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:58<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.0988)\n",
            "Training completed for epoch 4\n",
            "Training accuracy for epoch 4: 0.412125\n",
            "Validation started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:04<00:00, 170.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 4\n",
            "Validation accuracy for epoch 4: 0.43\n",
            "Training started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:55<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1095)\n",
            "Training completed for epoch 5\n",
            "Training accuracy for epoch 5: 0.4125\n",
            "Validation started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:04<00:00, 171.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 5\n",
            "Validation accuracy for epoch 5: 0.39875\n",
            "Training started for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:56<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1025)\n",
            "Training completed for epoch 6\n",
            "Training accuracy for epoch 6: 0.4175\n",
            "Validation started for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:03<00:00, 204.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 6\n",
            "Validation accuracy for epoch 6: 0.415\n",
            "Training started for epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:57<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1158)\n",
            "Training completed for epoch 7\n",
            "Training accuracy for epoch 7: 0.408125\n",
            "Validation started for epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:04<00:00, 196.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 7\n",
            "Validation accuracy for epoch 7: 0.41\n",
            "Training done to avoid overfitting!\n",
            "Best validation accuracy is: 0.415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}